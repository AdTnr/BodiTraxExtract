from playwright.sync_api import sync_playwright
import requests
from bs4 import BeautifulSoup
import json
import time

# URLs for each metric
BASE_URL = "https://member.boditrax.cloud"
LOGIN_URL = "https://identity.boditrax.cloud/Account/Login"
METRICS_URLS = {
    "weight": f"{BASE_URL}/Result/Weight",
    "fat": f"{BASE_URL}/Result/Fat",
    "muscle_mass": f"{BASE_URL}/Result/Muscle",
    "bmr": f"{BASE_URL}/Result/BasalMetabolicRate",
    "water_mass": f"{BASE_URL}/Result/Water",
    "metabolic_age": f"{BASE_URL}/Result/MetabolicAge",
    "bmi": f"{BASE_URL}/Result/BMI",
}

def login_with_playwright():
    """Opens a browser for manual login and returns the cookies once authenticated."""
    with sync_playwright() as p:
        browser = p.firefox.launch(headless=False)
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='en-US',
        )
        
        # Simplified stealth script
        context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined});")
        
        page = context.new_page()
        page.goto(BASE_URL)
        
        print("\nPlease log in manually in the browser window.")
        print("Script will continue automatically after login.")
        print("DO NOT close the browser window.\n")
        
        # Wait for successful login with timeout
        page.wait_for_url("**/member.boditrax.cloud/**", timeout=300000)
        time.sleep(2)
        
        # Set up session with cookies
        session = requests.Session()
        for cookie in context.cookies():
            session.cookies.set(cookie['name'], cookie['value'], domain=cookie['domain'])
        
        browser.close()
        return session

def scrape_metric(session, url):
    """Scrapes data from a specific metric page."""
    try:
        response = session.get(url)
        if response.status_code != 200:
            print(f"Failed to fetch {url}: Status {response.status_code}")
            return None
        
        soup = BeautifulSoup(response.text, "html.parser")
        metric_classes = ["current-metric", "kg-metric", "percentage-metric"]
        
        for class_name in metric_classes:
            if metric_element := soup.find("h3", class_=class_name):
                text = metric_element.text.strip()
                # Find the split point between value and unit
                split_point = next((i for i, char in enumerate(text) 
                                  if not (char.isdigit() or char in '.-')), len(text))
                
                return {
                    "value": text[:split_point].strip(),
                    "unit": text[split_point:].strip()
                }
        
        print(f"No metric found in {url}")
        return None
        
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return None

def calculate_lean_body_mass(metrics):
    """Calculate Lean Body Mass from weight and fat mass."""
    try:
        weight = float(metrics['weight']['value'])
        fat_mass = float(metrics['fat']['value'])
        lean_mass = weight - fat_mass
        
        return {
            "value": f"{lean_mass:.2f}",
            "unit": "kg"
        }
    except (KeyError, ValueError) as e:
        print(f"Error calculating lean body mass: {e}")
        return None

def scrape_all_metrics(session):
    """Scrapes all metrics and returns them as a dictionary."""
    metrics = {}
    for metric, url in METRICS_URLS.items():
        print(f"\nScraping {metric}...")
        data = scrape_metric(session, url)
        if data:
            metrics[metric] = data
        time.sleep(1)  # Add a small delay between requests
    
    # Calculate and add lean body mass
    if 'weight' in metrics and 'fat' in metrics:
        lean_mass = calculate_lean_body_mass(metrics)
        if lean_mass:
            metrics['lean_body_mass'] = lean_mass
            print(f"\nCalculated lean body mass: {lean_mass['value']} {lean_mass['unit']}")
    
    return metrics

def save_to_json(data, filename="boditrax_data.json"):
    """Saves the data to a JSON file."""
    with open(filename, "w") as f:
        json.dump(data, f, indent=4)
    print(f"Data saved to {filename}")

if __name__ == "__main__":
    try:
        # Get authenticated session through browser login
        print("Opening browser for login...")
        session = login_with_playwright()
        print("Successfully logged in!")
        
        # Scrape the metrics
        metrics_data = scrape_all_metrics(session)
        save_to_json(metrics_data)
    except Exception as e:
        print(f"Error: {e}")